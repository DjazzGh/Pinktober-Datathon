{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O-SrstgQlEw",
        "outputId": "16a92c54-6f66-4d91-fb39-fc6e5a675362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = '/content/drive/MyDrive/micro-club-pinktober-breast-cancer-detection.zip'\n"
      ],
      "metadata": {
        "id": "GXFBlRkXsYX3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/breast_cancer_data')  # extracts all files to this folder\n"
      ],
      "metadata": {
        "id": "BubldedxuAe5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Check 1: Does the main data folder exist?\n",
        "DATA_DIR = './breast_cancer_data'\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
        "\n",
        "print(f\"Checking existence of DATA_DIR: {os.path.exists(DATA_DIR)}\")\n",
        "print(f\"Checking existence of TRAIN_DIR: {os.path.exists(TRAIN_DIR)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aiRqfy_S-Cz",
        "outputId": "08efe95a-87fc-4686-b572-e3784a5e4197"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking existence of DATA_DIR: True\n",
            "Checking existence of TRAIN_DIR: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "\n",
        "# ⚠️ UPDATE THESE PATHS\n",
        "TRAIN_DIR = \"./breast_cancer_data/train\"\n",
        "TEST_DIR = \"./breast_cancer_data/test\"\n",
        "SUBMISSION_FILE = \"submission.csv\"\n",
        "\n",
        "# Model & Training Parameters\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "# FIX ATTEMPT: Reset to standard transfer learning LR (0.001)\n",
        "# since the model was stuck at 50% with the lower LR (0.0001).\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "\n",
        "# --- 2. LOAD & PREPARE DATA ---\n",
        "\n",
        "print(\"Loading and preparing data...\")\n",
        "\n",
        "# 1. Load Training and Validation Datasets\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "# 2. Get class names and map them to M/N\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Class names found (Index 0/1): {class_names}\")\n",
        "\n",
        "def map_to_submission_label(index):\n",
        "    # Determine the submission label (M or N) based on the class index\n",
        "    if class_names[index].lower() == 'malignant':\n",
        "        return 'M'\n",
        "    elif class_names[index].lower() == 'normal':\n",
        "        return 'N'\n",
        "    return '?'\n",
        "\n",
        "# 3. Create Data Augmentation layer\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomContrast(0.2),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "\n",
        "# 4. Define preprocessing functions (MobileNetV2 normalization)\n",
        "def preprocess_and_augment(image, label):\n",
        "    image = data_augmentation(image, training=True)\n",
        "    image = preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "def preprocess_only(image, label):\n",
        "    image = preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "# 5. Apply preprocessing and optimize pipelines\n",
        "train_ds = train_ds.map(preprocess_and_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(preprocess_only, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# --- 3. BUILD THE MODEL (TRANSFER LEARNING) ---\n",
        "\n",
        "print(\"Building model...\")\n",
        "\n",
        "# 1. Load pre-trained base model (MobileNetV2)\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=IMAGE_SIZE + (3,),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# 2. Start by FREEZING the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create the new classification head\n",
        "inputs = keras.Input(shape=IMAGE_SIZE + (3,))\n",
        "# NOTE: The redundant preprocess_input layer is REMOVED (the fix for 50% accuracy)\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.5)(x) # Increased Dropout for stability\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# 4. Combine into the final model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# 5. Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# --- 4. TRAIN THE MODEL ---\n",
        "\n",
        "print(\"Starting model training (Stage 1: Frozen Base)...\")\n",
        "\n",
        "# Use generous patience since training has been unstable\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=7,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ⚡ OPTIONAL STAGE 2: Fine-Tuning\n",
        "# Run this block ONLY if Stage 1 (above) results in accuracy > 65%\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Starting Stage 2: Fine-Tuning ---\")\n",
        "# Unfreeze the base model\n",
        "base_model.trainable = True\n",
        "\n",
        " # Freeze all layers except the last few blocks (e.g., last 20 layers)\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# # Re-compile the model with a very low learning rate\n",
        "model.compile(\n",
        "     optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE / 10), # e.g., 0.00001\n",
        "     loss='binary_crossentropy',\n",
        "     metrics=['accuracy']\n",
        " )\n",
        "\n",
        "# # Continue training for a few more epochs\n",
        "history_ft = model.fit(\n",
        "     train_ds,\n",
        "    epochs=EPOCHS + 5, # Run for 5 extra epochs\n",
        "     initial_epoch=history.epoch[-1], # Start from where Stage 1 left off\n",
        "     validation_data=val_ds,\n",
        "     callbacks=callbacks\n",
        " )\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# --- 5. MAKE PREDICTIONS AND CREATE SUBMISSION FILE ---\n",
        "\n",
        "print(f\"\\nLoading test data from: {TEST_DIR}\")\n",
        "\n",
        "# 1. Load the test dataset (no labels, no shuffle)\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    TEST_DIR,\n",
        "    labels=None,\n",
        "    shuffle=False,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# 2. Get the filenames in the correct order\n",
        "test_filenames = test_ds.file_paths\n",
        "test_filenames = [os.path.basename(f) for f in test_filenames]\n",
        "\n",
        "# 3. Preprocess the test data\n",
        "test_ds = test_ds.map(lambda image: preprocess_input(image), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# 4. Make predictions\n",
        "print(\"Running predictions on test set...\")\n",
        "raw_predictions = model.predict(test_ds)\n",
        "\n",
        "# 5. Convert probabilities to class indices (0 or 1)\n",
        "predicted_indices = (raw_predictions > 0.5).astype(int)\n",
        "\n",
        "# 6. Map indices to the final 'M' or 'N' submission labels\n",
        "submission_labels = [map_to_submission_label(i[0]) for i in predicted_indices]\n",
        "\n",
        "# 7. Create the DataFrame and save to CSV\n",
        "submission_df = pd.DataFrame({\n",
        "    # FIX HERE: Change 'image' to 'ID' (or 'id')\n",
        "    'ID': test_filenames,\n",
        "    'label': submission_labels\n",
        "})\n",
        "\n",
        "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
        "\n",
        "print(\"\\n-------------------------------------------------\")\n",
        "print(f\"SUCCESS! Submission file created: {SUBMISSION_FILE}\")\n",
        "print(\"First 10 lines of submission (Check Header!):\")\n",
        "print(submission_df.head(10))\n",
        "print(\"-------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7ZcR8z8IPEUj",
        "outputId": "b1adb9ff-0d67-406d-eb5a-005eb090310b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Found 700 files belonging to 2 classes.\n",
            "Using 560 files for training.\n",
            "Found 700 files belonging to 2 classes.\n",
            "Using 140 files for validation.\n",
            "Class names found (Index 0/1): ['malignant', 'normal']\n",
            "Building model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,259,265\u001b[0m (8.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,259,265</span> (8.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training (Stage 1: Frozen Base)...\n",
            "Epoch 1/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - accuracy: 0.4859 - loss: 0.9174 - val_accuracy: 0.5357 - val_loss: 0.7331\n",
            "Epoch 2/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 468ms/step - accuracy: 0.5045 - loss: 0.8770 - val_accuracy: 0.5500 - val_loss: 0.7303\n",
            "Epoch 3/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 428ms/step - accuracy: 0.5210 - loss: 0.8330 - val_accuracy: 0.5071 - val_loss: 0.7189\n",
            "Epoch 4/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 372ms/step - accuracy: 0.5170 - loss: 0.8299 - val_accuracy: 0.5143 - val_loss: 0.7154\n",
            "Epoch 5/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 476ms/step - accuracy: 0.5563 - loss: 0.7566 - val_accuracy: 0.5071 - val_loss: 0.7277\n",
            "Epoch 6/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 395ms/step - accuracy: 0.4924 - loss: 0.8112 - val_accuracy: 0.5143 - val_loss: 0.7257\n",
            "Epoch 7/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 430ms/step - accuracy: 0.5479 - loss: 0.7479 - val_accuracy: 0.5357 - val_loss: 0.7034\n",
            "Epoch 8/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 464ms/step - accuracy: 0.5313 - loss: 0.7646 - val_accuracy: 0.5286 - val_loss: 0.7018\n",
            "Epoch 9/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 388ms/step - accuracy: 0.5338 - loss: 0.7827 - val_accuracy: 0.5429 - val_loss: 0.7053\n",
            "\n",
            "--- Starting Stage 2: Fine-Tuning ---\n",
            "Epoch 9/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.5125 - loss: 0.8107 - val_accuracy: 0.4786 - val_loss: 0.7402\n",
            "Epoch 10/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 443ms/step - accuracy: 0.5610 - loss: 0.7346 - val_accuracy: 0.5286 - val_loss: 0.8926\n",
            "Epoch 11/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 398ms/step - accuracy: 0.5258 - loss: 0.7471 - val_accuracy: 0.5643 - val_loss: 0.7452\n",
            "Epoch 12/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 472ms/step - accuracy: 0.6008 - loss: 0.6625 - val_accuracy: 0.5500 - val_loss: 0.7699\n",
            "Epoch 13/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 400ms/step - accuracy: 0.6016 - loss: 0.6774 - val_accuracy: 0.5286 - val_loss: 0.7900\n",
            "Epoch 14/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 404ms/step - accuracy: 0.5908 - loss: 0.6625 - val_accuracy: 0.5429 - val_loss: 0.8018\n",
            "Epoch 15/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 467ms/step - accuracy: 0.5917 - loss: 0.6903 - val_accuracy: 0.5786 - val_loss: 0.7452\n",
            "Epoch 16/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 374ms/step - accuracy: 0.5821 - loss: 0.6714 - val_accuracy: 0.5929 - val_loss: 0.7408\n",
            "Epoch 17/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 480ms/step - accuracy: 0.6276 - loss: 0.6710 - val_accuracy: 0.5714 - val_loss: 0.7642\n",
            "Epoch 18/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 388ms/step - accuracy: 0.6447 - loss: 0.6332 - val_accuracy: 0.5571 - val_loss: 0.8856\n",
            "Epoch 19/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 438ms/step - accuracy: 0.6440 - loss: 0.6314 - val_accuracy: 0.5500 - val_loss: 0.8810\n",
            "Epoch 20/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 479ms/step - accuracy: 0.6032 - loss: 0.6585 - val_accuracy: 0.5500 - val_loss: 0.9601\n",
            "Epoch 21/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 370ms/step - accuracy: 0.6455 - loss: 0.6139 - val_accuracy: 0.5500 - val_loss: 0.8980\n",
            "Epoch 22/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 483ms/step - accuracy: 0.6673 - loss: 0.6031 - val_accuracy: 0.5643 - val_loss: 0.9147\n",
            "Epoch 23/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 459ms/step - accuracy: 0.6536 - loss: 0.6403 - val_accuracy: 0.5429 - val_loss: 1.0071\n",
            "\n",
            "Loading test data from: ./breast_cancer_data/test\n",
            "Found 300 files.\n",
            "Running predictions on test set...\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 484ms/step\n",
            "\n",
            "-------------------------------------------------\n",
            "SUCCESS! Submission file created: submission.csv\n",
            "First 10 lines of submission (Check Header!):\n",
            "           ID label\n",
            "0  101837.jpg     N\n",
            "1  102480.jpg     N\n",
            "2  105295.jpg     M\n",
            "3  107574.jpg     M\n",
            "4  113739.jpg     N\n",
            "5   11652.jpg     M\n",
            "6  116761.jpg     N\n",
            "7  123047.jpg     N\n",
            "8  132435.jpg     M\n",
            "9  134255.jpg     N\n",
            "-------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
